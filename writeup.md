# The Deep Writing Project
CS 4501 Computer Vision @UVa

We would like to have our final writeup posted on the course website, especially with the following figure proudly generated by our model :

![header](./img/example.png)

Acknowledge
-----------
Thanks to Prof. Connelly Barnes and Prof. Kaiwei Chang for instructions and feedback on this class project. Part of the experiments are performance based on the UVa CS department **Artemis** Nvidia GPU node.

This work is mainly based on the paper: ***Graves, Alex. "Generating sequences with recurrent neural networks." arXiv preprint arXiv:1308.0850 (2013)*** [Link](https://arxiv.org/pdf/1308.0850.pdf) 

For the model implementation, we also referenced the model structure from [RNNLIB](https://sourceforge.net/projects/rnnl/) by Alex Graves with C++. 

The  model is trained on the dataset  [IAM online handwriting dataset](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database), with additional records of sequential pen-tip locations in each stroke, as opposed to its offline handwriting dataset, where only handwriting image is available.

Introduction
-----------------------
Every person has an unique handwriting style, and sometime it can expose the person’s identity especially with some machine learning exploits. Therefore, in many cases when hand-writing is required, it is helpful to use the model generated hand-writing to hidden one’s identity.

Meanwhile, as the accented speech generator could improve the human-computer interaction, a stylish hand-written generator with indeterministic nature would also become a useful tool to create different kinds of human friendly user experiences.

Implementation
--------------
Our work focuses on the modeling the probability distribution of sequential data from the handwriting stroke data, introduced in Alex Graves's paper. While the traditional machine learning approach only mimic the handwriting features without awareness of written context, our network also takes the letters and short word as additional training features to model the global features of handwriting style so that the model is sufficient to perform the advanced handwriting synthesis tasks. 

The Synthesis Network is illustrated as following in the paper, the solid line reprensents the connections, and the dashed line represents the prediction.
![Figure from Alex Graves' Paper](./img/architecture.png)

To better pipeline our training, we separate the Graves' model into three individual parts:

- The Long Short-Term Memory (LSTM) Cell  
The core of our sequential probability model are n-layers of Long Short-Term Memory (LSTM) Recurrent Neural Networks (The author uses three layers in the paper). And we use the tensorFlow library API for this model.  

- The Mixture Density Network (MDN)  
The Mixture Density Network is such network that can measure their own uncertainty. Its output parameters are several multivariate Gaussian components, and it also estimate the distributions for each of these parameters so that the output value can be drawn from these particular component’s distribution.

- The Attention Mechanism    
In order to get the information about which characters make up this sentence, the model uses a differentiable attention mechanism. Specifically, it is a Gaussian convolution over a one-hot(vector of alphabet length) letter encoding. The convolution operation is like a soft window through text label sequence which the handwriting model can look at a certain n-gram of characters, i.e. the letters ‘he’, 'el', 'll', 'lo' in the word ‘hello’. And we use these processed info as additional features we feed into the model.

The other useful implementation to make handwriting more readable is to use a biased sampling, as the handwriting dataset gives the style label. The intuitive reason behind is that the unbiased sampling tends to mix up all styles and make the generated handwriting unreadable. 


Milestone
---------
- Machine learning framework exploration
- Tensorflow setup & GPU support
- Dataset parsing and biased sampling
- Stacked LSTM layers, MDN, Attention Mechanism implementation
- Model inference & training functionality separation
- Jupyter notebook interaction interface with detailed explanations
- Experiments on fine tuning parameters. 

Results
-------
The demo part is available in [inference.ipynb](./inference.ipynb), and the training part is available in [train.ipynb](./train.ipynb) with Python 3.5, Tensorflow 1.0, Jupyter Notebook 4.1

The author of RNNlib suggested the parameter setting:

- Alphabet  list= " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
- 3 Layers of 400 LSTM cells
- Initial adaptive weight noise with std. devs. of 0.075
- 20 bivariate Gaussian mixture component in MDN
- 10 sigma in the soft window of Attention Mechanism 
- Trained with SGD optimizer ***rmsprop*** 

<!-- Besides using the parameter above, we also trained the model with different several different parameter settings.  -->
 ![Hello](./img/hello.png)
 ![World](./img/world.png)
 
 
Conclusion
----------
The paper also briefly explained the prime sample strategy used to learn the handwriting style by 'prime' the network with data sequence of a certain style to avoid retraining the model. Even though we have classified unique handwriting styles with such strategy in the real training, the generated result is not clear enough to be as readable as original style sample in the dataset.

As we can see from the result above, the handwriting results on some letters are especially bad. For example, "Q", "Z", the reason behind is probably that the capitalized letter is  probably rare in the dataset. "T","t" the reason behind is probably people use various different stroke pattern for writing such letter. For letters like "M", "W", "N", people used to write random curve that is hard for the model to find patterns. And the handwriting uppercase letter are usually worse than lowercase letter,  because uppercase letter can only exist in the start of the sentence most of time. 

The other limitation of this project is that the punctuation is not taken into consideration in the training while the handwriting dataset did include the punctuation as text annotation. The reason we do this is that the punctuation handwriting is not consistent and so hard for the model to predict.

Since the inference procedure involves random variables, the model could miss a couple letters in producing a sentence. However, we cannot simply remove these random variable, because they play a curial role in the sampling. Therefore, it could take several attempts to produce a perfect writing. In the future work, it is possible to include a certain adversarial  mechanism with some recognition network in the output layer to force the model to generate a correct output.

Contribution
------------
- Charlie Wu ([jw7jb@virginia.edu](mailto:jw7jb@virginia.edu))
  - Writeup
  - Paper review
  - Jupyter interface 
- Jerry Sun ([ys7va@virginia.edu](mailto:ys7va@virginia.edu))
  - Learning model setup
  - util function setup
  - Code management 
- Tony Qiu ([tq7bw@virginia.edu](mailto:tq7bw@virginia.edu))
   - Data I/O
   - Hardware support
   - Results evaluation
 
 ![Charlie](./img/charlie.png)

![Jerry](./img/jerry.png)

![Tong](./img/tony.png)
